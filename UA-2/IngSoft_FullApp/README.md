[![Review Assignment Due Date](https://classroom.github.com/assets/deadline-readme-button-22041afd0340ce965d47ae6ef1cefeee28c7c493a6346c4f15d667ab976d596c.svg)](https://classroom.github.com/a/-1JTPT80)

## DescripciÃ³n General

Esta guÃ­a describe cÃ³mo implementar una aplicaciÃ³n web inteligente para e-commerce que permite bÃºsqueda de productos por texto o por imagen, utilizando modelos de IA pre-entrenados. La aplicaciÃ³n estÃ¡ compuesta por varios servicios (backend, servicio de inferencia, frontend) que se comunicarÃ¡n entre sÃ­. Los estudiantes deberÃ¡n integrar componentes de FastAPI, Celery, Redis, MariaDB, ONNX y Gradio, siguiendo la arquitectura propuesta. Al finalizar, se podrÃ¡ desplegar toda la soluciÃ³n con Docker Compose y verificar su correcto funcionamiento de extremo a extremo.

## Funciones clave de la aplicaciÃ³n

* **BÃºsqueda por texto:** El usuario ingresa texto (p.ej. nombre de producto o categorÃ­a) y la aplicaciÃ³n devuelve productos relevantes desde la base de datos.
* **BÃºsqueda por imagen:** El usuario suministra una imagen de un producto; un modelo de IA clasificarÃ¡ la imagen para determinar su categorÃ­a, y se listarÃ¡n productos similares de esa categorÃ­a.
* **Modelos IA pre-entrenados:** Se usarÃ¡n modelos ya entrenados (por ejemplo, MobileNet v2 para clasificaciÃ³n de imÃ¡genes) sin necesidad de entrenarlos desde cero.
* **Frontend interactivo:** Una interfaz web (con Gradio) permitirÃ¡ a los usuarios cargar imÃ¡genes o escribir consultas de texto y visualizar los resultados.
* **EjecuciÃ³n del modelo en servidor y en navegador:** El servicio de inferencia ejecuta el modelo en el backend (usando ONNX Runtime en Python), y adicionalmente se muestra cÃ³mo podrÃ­a integrarse el modelo en el navegador usando onnxruntime-web para realizar inferencia directamente en el cliente (WebAssembly).
* **Despliegue en contenedores:** Todos los componentes (base de datos, backend, servicio de IA, UI) se orquestan mediante Docker Compose para facilitar la ejecuciÃ³n de la aplicaciÃ³n completa con un solo comando.

## Arquitectura

La aplicaciÃ³n se divide en tres servicios principales y una base de datos, comunicÃ¡ndose en una arquitectura de microservicios:
* **Backend Principal (FastAPI):** Proporciona una API REST para el cliente. Gestiona las consultas de texto directamente (consultando la BD) y delega las consultas por imagen al servicio de inferencia. TambiÃ©n coordina la obtenciÃ³n de resultados y los envÃ­a al frontend.
    * **Base de Datos (MariaDB):** Almacena la informaciÃ³n de productos y categorÃ­as. Se usarÃ¡ MariaDB (MySQL) para persistir datos de ejemplo (productos simulados).
* **Servicio de Inferencia (FastAPI + Celery):** Microservicio dedicado a tareas de IA. Expone endpoints (por ejemplo, para *health check* o procesamiento inmediato) y define tareas Celery para ejecutar la inferencia con modelos de IA de forma asÃ­ncrona. Utiliza ONNX Runtime para ejecutar un modelo de clasificaciÃ³n de imÃ¡genes. Estas tareas se distribuyen a travÃ©s de Redis que actÃºa como *message broker* y almacÃ©n de resultados.
* **Frontend (Gradio):** Interfaz web que permite al usuario final interactuar con la aplicaciÃ³n de forma sencilla. Incluye componentes para ingresar texto o imÃ¡genes y muestra los productos recuperados. El frontend se comunica con el backend principal (vÃ­a solicitudes HTTP internas) para enviar consultas y obtener resultados. AdemÃ¡s, puede integrar cÃ³digo JavaScript/TypeScript para demostrar inferencia en el navegador con onnxruntime-web.

La comunicaciÃ³n entre servicios sigue este flujo general:

1. Consulta de texto: El frontend envÃ­a la consulta al endpoint REST del backend. El backend busca en la base de datos los productos cuyo nombre o descripciÃ³n coinciden y devuelve la lista al frontend para mostrar.
2.	Consulta de imagen: El frontend envÃ­a la imagen al backend (vÃ­a un endpoint de carga de archivos). El backend llama al servicio de inferencia para clasificar la imagen (ya sea enviando una tarea Celery o llamando a un endpoint interno). Una vez obtenida la categorÃ­a predicha, el backend consulta la base de datos por productos de esa categorÃ­a y devuelve la lista al frontend.
3.	Inferencia en navegador (opcional): Alternativamente, el frontend puede cargar el modelo ONNX en el navegador y ejecutar la clasificaciÃ³n localmente (WebAssembly), luego enviar solo el resultado de categorÃ­a al backend para la bÃºsqueda en BD. Esto reduce carga en el servidor y demuestra funcionamiento sin backend 

## Estructura inicial del proyecto

```bash
ğŸ“¦ proyecto-ecommerce-ia
â”œâ”€â”€ README.md
â”œâ”€â”€ backend
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ app
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ api
â”‚       â”‚   â””â”€â”€ webhook.py
â”‚       â”œâ”€â”€ controllers
â”‚       â”œâ”€â”€ db
â”‚       â”‚   â”œâ”€â”€ entities
â”‚       â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”‚   â”œâ”€â”€ category.py
â”‚       â”‚   â”‚   â””â”€â”€ product.py
â”‚       â”‚   â””â”€â”€ registry.py
â”‚       â”œâ”€â”€ main.py
â”‚       â”œâ”€â”€ services
â”‚       â””â”€â”€ state.py
â”œâ”€â”€ data
â”‚   â””â”€â”€ init.sql
â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ frontend
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ app.py
â”‚   â””â”€â”€ requirements.txt
â””â”€â”€ inference
    â”œâ”€â”€ Dockerfile
    â”œâ”€â”€ Dockerfile.worker
    â”œâ”€â”€ README.md
    â”œâ”€â”€ app
    â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”œâ”€â”€ assets
    â”‚   â”œâ”€â”€ main.py
    â”‚   â”œâ”€â”€ models
    â”‚   â”‚   â”œâ”€â”€ __init__.py
    â”‚   â”‚   â””â”€â”€ squeezenet.py
    â”‚   â””â”€â”€ tasks.py
    â””â”€â”€ requirements.txt
```

## EvaluaciÃ³n

Esta prÃ¡ctica **se puede realizar de forma individual o en grupos de dos personas**. En caso de realizarla en grupo, ambos miembros deberÃ¡n participar activamente en el desarrollo de la prÃ¡ctica y conocer en detalle el cÃ³digo implementado, y su participaciÃ³n deberÃ¡ quedar reflejada en el repositorio.

La evaluaciÃ³n de la prÃ¡ctica se realizarÃ¡ mediante **pruebas automÃ¡ticas** y una **revisiÃ³n manual** del cÃ³digo que valorarÃ¡ el diseÃ±o de las clases y mÃ©todos, la claridad y organizaciÃ³n del cÃ³digo, y el uso de buenas prÃ¡cticas de programaciÃ³n siguiendo los conceptos vistos en la asignatura.

| Concepto                                                                                          | Peso |
|---------------------------------------------------------------------------------------------------|------|
| ImplementaciÃ³n del servicio de inferencia (FastAPI + ONNX + Celery)                               | 25%  |
| ComunicaciÃ³n entre backend e inferencia (tareas encoladas, seguimiento de tareas...)              | 20%  |
| ComunicaciÃ³n entre frontend y backend (interacciÃ³n vÃ­a API, polling, estados, errores)            | 15%  |
| ImplementaciÃ³n correcta de los endpoints del backend (FastAPI)                                    | 10%  |
| RealizaciÃ³n de pruebas (unitarias, de integraciÃ³n y cobertura)                                    | 10%  |
| Estructura modular del proyecto y uso de buenas prÃ¡cticas de organizaciÃ³n de cÃ³digo               | 5%   |
| DocumentaciÃ³n tÃ©cnica en cada mÃ³dulo                                                              | 5%   |
| Uso de Docker para el despliegue de la aplicaciÃ³n completa                                        | 5%   |
| ImplementaciÃ³n de un frontend usable e interpretable                                              | 5%   |

## Recomendaciones

### ComunicaciÃ³n entre backend y frontend para obtener resultados de inferencia

Cuando el usuario envÃ­a una imagen o descripciÃ³n a travÃ©s de la interfaz, el backend encola una tarea de inferencia. Sin embargo, el resultado de esta tarea no estÃ¡ disponible inmediatamente, ya que el procesamiento lo realiza un `worker` de forma asÃ­ncrona. Para manejar este flujo, deberÃ¡n implementar un sistema que permita al frontend saber cuÃ¡ndo estÃ¡ listo el resultado.

1.	Al crear la tarea de inferencia, el backend debe devolver un identificador Ãºnico (`task_id`) al frontend.
2.	El frontend debe mostrar un estado de carga (loader o indicador visual) mientras espera la respuesta.
3.	Para consultar si la tarea ya ha sido completada, el frontend deberÃ¡ hacer peticiones periÃ³dicas (polling) a endpoint del backend que verifique el estado de la tarea (usando el `task_id`).
    - `/tasks/{task_id}/result` (GET): Devuelve el resultado de la tarea si estÃ¡ lista, o un mensaje indicando que aÃºn no ha finalizado.
4.	Este endpoint debe funcionar de la siguiente forma:
    - Si la tarea todavÃ­a no estÃ¡ lista, debe responder con un cÃ³digo HTTP 202 Accepted y un mensaje indicando que el resultado no estÃ¡ disponible.
    - Si la tarea ya ha sido completada, debe responder con el resultado (por ejemplo, la lista de predicciones) y un cÃ³digo HTTP 200 OK.
5.  El servidor de inferencia notificarÃ¡ al backend principal cuando una tarea estÃ© completada, a travÃ©s de un webhook ya implementado (`/webhook/task_completed`). Este webhook almacena el resultado en una estructura temporal del backend (`result_store`).